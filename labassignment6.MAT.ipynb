{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 6: Creating and Connecting to Databases\n",
    "## DS 6001\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n",
    "\n",
    "**Please note: you will not be able to use Rivanna for this lab as Rivanna is not set up to work with Docker or with Databases. Problem 1 will guide you through the steps to get databases running on your local system. This can be tricky, so if you have questions or would like some help, please let me know.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import psycopg\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pymongo\n",
    "from bson.json_util import loads, dumps\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Many database systems run as external software on a computer. Some of the software is commercial (Oracle, Microsoft SQL Server), with limited free use and expensive enterprise level licenses. Other database software is entirely free and open source (MySQL, Postgres, SQlite, MongoDB). Regardless of whether the software is free or not, databases are notoriously difficult to install and start using. More than other kinds of software, database systems seem to run into many errors that are specific to the operating system of your computer. We used to ask students to install Postgres, MySQL, and Mongo, and it was a nightmare because everyone hit some sort of roadblock and those roadblocks were different for every student.\n",
    "\n",
    "That's just how databases seem to be! Not just for students, but for everyone!\n",
    "\n",
    "A growing standard practice in industry is to use **containers** to deploy database systems. Please read the [discussion of containers in the course textbook],(https://jkropko.github.io/surfing-the-data-pipeline/docker.html) if you haven't yet done so to familiarize yourself with the purpose and uses of containers and Docker.\n",
    "\n",
    "This lab will guide you through the installation steps for the software you need to run database systems on your computer (problem 1), build a relational schema (problem 2), document those databases (problem 3), and connect to them through Python with (fingers crossed) as few problems as possible (problem 4 for relational databases, problem 5 for a NoSQL database).\n",
    "\n",
    "With the exception of SQLite (which operates as a Python package), database systems run as external software that must be installed and run on your computer. To make the installation steps easier, you will need some configuration files that I wrote and saved in a GitHub repository. Open your terminal and then type\n",
    "```\n",
    "git clone https://github.com/jkropko/ds6001databases\n",
    "```\n",
    "If this command works, it will create a new directory within your current folder called \"ds6001databases\".\n",
    "\n",
    "* Check that this folder exists and contains the following files: LICENSE, README.md, compose.yaml, and requirements.txt. (Don't worry if there are a couple extra files.)\n",
    "\n",
    "* To make things easier, save the notebook file you will be using for your Lab 6 work inside the \"ds6001databases\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "After cloning the ds6001databases repository, open and examine the file called compose.yaml. Then write answers to the following questions based on your reading of the section on [Docker compose files](https://jkropko.github.io/surfing-the-data-pipeline/docker.html#docker-compose-files) in the textbook:\n",
    "\n",
    "1. How many containers are being launched when we execute this compose.yaml file?\n",
    "\n",
    "2. What does each of the containers do?\n",
    "\n",
    "3. Docker is building these containers from Docker image files that it downloads from a website. On what website are these Docker images stored? (You can provide the general name of this website, no need for the specific URLs)\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part a\n",
    "1. Three containers are being launched, mysql, postgres and mongo.\n",
    "\n",
    "2. \n",
    "- Specifies the version of MySQL, Uses .env for credentials, maps port 3306:3306, persists data with the mysqldata:/var/lib/mysql volume\n",
    "- Specifies the versin of postgres, Uses .env for credentials, maps port 5432:5432, persists data with the postgresdata:/var/lib/postgresql/data volume\n",
    "- Specifies the version of Mongo, Uses .env for credentials, maps port 27017:27017, persists data with the mongodata:/data/db volume\n",
    "\n",
    "3. Docker Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "If you haven't yet done so, install Docker Desktop on your computer. Go to https://www.docker.com/products/docker-desktop/ and click on the Download button, making sure the operating system listed matches the operating system of your computer (for Apple users, make sure you get the correct version for your computer's processing chip. Click the Apple icon in the top-left corner of your screen, select \"About This Mac\", and see whether Intel or Apple M1 is listed under Chip).\n",
    "\n",
    "Once Docker Desktop is installed, find the Docker Desktop program on your computer and run it.\n",
    "\n",
    "To confirm that Docker Desktop is running, open a terminal and type `docker version`. Copy-and-paste what you see on the screen into your notebook.\n",
    "\n",
    "If your terminal hangs without displaying anything, that likely means that Docker Desktop is not running or not properly installed. Double check that the Docker Desktop is running. If your terminal is hanging, you can press Control + C to regain access to the command prompt.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(base) mauriciotorres@Mauricios-MacBook-Pro ds6001databases2025 % docker version\n",
    "Client:\n",
    " Version:           28.4.0\n",
    " API version:       1.51\n",
    " Go version:        go1.24.7\n",
    " Git commit:        d8eb465\n",
    " Built:             Wed Sep  3 20:56:26 2025\n",
    " OS/Arch:           darwin/arm64\n",
    " Context:           desktop-linux\n",
    "Cannot connect to the Docker daemon at unix:///Users/mauriciotorres/.docker/run/docker.sock. Is the docker daemon running?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Inside your \"ds6001databases\" folder, create a file named `.env`. Inside the .env file you need to choose passwords for the MySQL, PostgreSQL, and MongoDB databases, so type\n",
    "```\n",
    "MYSQL_ROOT_PASSWORD=password1\n",
    "POSTGRES_PASSWORD=password2\n",
    "MONGO_INITDB_ROOT_PASSWORD=password3\n",
    "MONGO_INITDB_ROOT_USERNAME=mongo\n",
    "mongo_init_db = mongodb\n",
    "MYSQL_DATABASE=mysql\n",
    "```\n",
    "Change the passwords on the first three lines to whatever you want, but DON'T USE THE @ SYMBOL as that will cause problems. Leave the fourth, fifth, and sixth lines alone, as well as the names of each environmental variable.\n",
    "\n",
    "Then run this line of Python code:\n",
    "```\n",
    "dotenv.load_dotenv()\n",
    "```\n",
    "If the .env file has successfully been created and saved in the correct location, the output of this code will be `True`. If `False`, double check the name and location of your file.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is True, so the .env file has been created and saved in the correct location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "In the terminal, make sure you are in the \"ds6001databases\" folder (you can check by typing `pwd`. If not, then use `cd` to navigate to the \"ds6001databases\" folder). Then type\n",
    "```\n",
    "docker compose up\n",
    "```\n",
    "This command launches all of the containers for running each of the database systems. If successful, you will see a long stream of output with messages that begin `ds6001databases-postgres-1`, `ds6001databases-mysql-1`, and `ds6001databases-mongo-1`. Copy and paste below the first six lines of this output. (Apologies for all the scrolling you might have to do!)\n",
    "\n",
    "If you receive an error message or if the terminal hangs without displaying output, you will likely need to double-check that parts a, b, and c can still be completed successfully, and that you are working within the \"ds6001databases\" folder via the terminal.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(base) mauriciotorres@Mauricios-MacBook-Pro ds6001databases2025 % docker compose up\n",
    "[+] Running 7/7\n",
    " ✔ Network ds6001databases2025_default        Created                                                            0.0s \n",
    " ✔ Volume \"ds6001databases2025_mysqldata\"     Created                                                            0.0s \n",
    " ✔ Volume \"ds6001databases2025_postgresdata\"  Created                                                            0.0s \n",
    " ✔ Volume \"ds6001databases2025_mongodata\"     Created                                                            0.0s \n",
    " ✔ Container ds6001databases2025-mysql-1      Created                                                            0.0s \n",
    " ✔ Container ds6001databases2025-postgres-1   Created                                                            0.1s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "To confirm that the databases are running on your system, run the following Python code. \n",
    "```\n",
    "# 1. Load needed environment variables\n",
    "dotenv.load_dotenv()\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "MONGO_INITDB_ROOT_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME')\n",
    "MONGO_INITDB_ROOT_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD')\n",
    "MYSQL_ROOT_PASSWORD = os.getenv('MYSQL_ROOT_PASSWORD')\n",
    "\n",
    "# 2. Test MySQL\n",
    "dbserver = mysql.connector.connect(\n",
    "    user='root', \n",
    "    password=MYSQL_ROOT_PASSWORD, \n",
    "    host='localhost',\n",
    "    port='3306',\n",
    "    #auth_plugin='mysql_native_password'\n",
    ")\n",
    "\n",
    "# 3. Test PostgreSQL\n",
    "dbserver = psycopg.connect(\n",
    "    user='postgres', \n",
    "    password=POSTGRES_PASSWORD, \n",
    "    host='localhost',\n",
    "    port = '5432'\n",
    ")\n",
    "dbserver.autocommit = True\n",
    "\n",
    "# 4. Test MongoDB\n",
    "myclient = pymongo.MongoClient(f\"mongodb://{MONGO_INITDB_ROOT_USERNAME}:{MONGO_INITDB_ROOT_PASSWORD}@localhost:27017/\")\n",
    "myclient.list_databases()\n",
    "```\n",
    "\n",
    "If this code runs without error, you will see output that looks something like\n",
    "```\n",
    "<pymongo.synchronous.command_cursor.CommandCursor at 0x177d8b380>\n",
    "```\n",
    "and you are all set for working with these databases using Python.\n",
    "\n",
    "If you do see an error, then you will need to troubleshoot. A few things to try:\n",
    "\n",
    "* Double check that your work for parts a, b, c, and d can still run successfully.\n",
    "\n",
    "* Read the error to determine which of the databases is causing the error, to narrow the problem down. Comment-out or delete this part of the code to see if the other databases are working.\n",
    "\n",
    "* Check the output in the terminal where you issued the `docker compose up` command. Scroll up and down, and look for any lines that write that a container has exited.\n",
    "\n",
    "* Try a reset of the containers by pressing Control+C in the terminal displaying Docker output, then `docker compose down`, then `docker compose up` again. Then try running the Python code again.\n",
    "\n",
    "* Sometimes (especially if you've changed the database passwords at some point) incorrect passwords get saved in the Docker volumes and do not change when you make a change to your .env file. To fix this, press Control+C in the terminal displaying Docker output, then `docker compose down`. Then open the Docker Desktop, click on volumes, and find and delete the volumes associated with the \"ds6001databases\" containers. Then return to the terminal,type `docker compose up`, and try again.\n",
    "\n",
    "* If all else fails, you are far from the only one to hit a roadblock at this stage. Send me or one of the TAs a message and we can help.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.synchronous.command_cursor.CommandCursor at 0x1255a2660>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load needed environment variables\n",
    "dotenv.load_dotenv()\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "MONGO_INITDB_ROOT_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME')\n",
    "MONGO_INITDB_ROOT_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD')\n",
    "MYSQL_ROOT_PASSWORD = os.getenv('MYSQL_ROOT_PASSWORD')\n",
    "\n",
    "# 2. Test MySQL\n",
    "dbserver = mysql.connector.connect(\n",
    "    user='root', \n",
    "    password=MYSQL_ROOT_PASSWORD, \n",
    "    host='localhost',\n",
    "    port='3306',\n",
    "    #auth_plugin='mysql_native_password'\n",
    ")\n",
    "\n",
    "# 3. Test PostgreSQL\n",
    "dbserver = psycopg.connect(\n",
    "    user='postgres', \n",
    "    password=POSTGRES_PASSWORD, \n",
    "    host='localhost',\n",
    "    port = '5432'\n",
    ")\n",
    "dbserver.autocommit = True\n",
    "\n",
    "# 4. Test MongoDB\n",
    "myclient = pymongo.MongoClient(f\"mongodb://{MONGO_INITDB_ROOT_USERNAME}:{MONGO_INITDB_ROOT_PASSWORD}@localhost:27017/\")\n",
    "myclient.list_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 \n",
    "For this problem, I created fake data that represents patients who saw a doctor and received one or more prescriptions. Your goal in this problem is to reorganize the data into a database schema that conforms to E. F. Codd's 3rd normal form, and to document the resulting relational database using an ER diagram.\n",
    "\n",
    "Wrapping your head around the normal form rules and building and documenting a database schema can be very difficult. First, grant yourself a lot of grace and give yourself plenty of time to take on this problem. Mastering database schema and documentation are central skills that distinguish a data engineer from a data scientist or analyst, so it is worth taking time to think about this task.\n",
    "\n",
    "The data are listed on the following Google sheet: https://docs.google.com/spreadsheets/d/11wUk5dg39lmI-_O0SQ0M5_5VGgzuGJKvSyMq4Tyxc9M/edit?usp=sharing. For this problem you will be manipulating the data. I only included 10 rows to enable you to use any method you like for working with the data, including point-and-click data manipulation on Google sheets or Excel. Or you can use `pandas` if you like. We are emphasizing the product over the process for now. \n",
    "\n",
    "There are two tabs on this spreadsheet. The first tab contains the data and the second tab contains column descriptions and notes. These notes (some of which are unrealistic, but needed for this exercise) are:\n",
    "\n",
    "* No two patients have the same name and date of birth, so `patient_name` and `date_of_birth` together uniquely identify a patient.\n",
    "\n",
    "* No two drugs share the same brand name, so `prescribed_drug` uniquely identifies the drug.\n",
    "\n",
    "* The data contain one row for every patient/prescription combination, and no patient receives more than one prescription for the same drug. So `patient_name`, `date_of_birth`, and `prescribed_drug` comprise a primary key in the original data.\n",
    "\n",
    "* No two physicians in the data share the same name, so `prescribing_physician` uniquely identifies physicians.\n",
    "\n",
    "* Patients do not change their insurance provider.\n",
    "\n",
    "* Every physician works at exactly one hospital, no two hospitals share the same name, and every hospital exists at exactly one location.\n",
    "\n",
    "Some things to keep in mind for this problem:\n",
    "\n",
    "1. Remember that the first rule for 2nd normal form is that the data are in 1st normal form, and the first rule for 3rd normal form is that the data are already in 2nd normal form. So please focus on meeting the rules for 1st, then 2nd, then 3rd normal form *in that order*. \n",
    "\n",
    "2. Some students have never worked with databases before, but some have. If you have seen databases before, you have some ideas about how database schema tend to look. But if you arrange the data to match an idea of how databases often look, you will more than likely over-engineer the database by creating too many tables or unnecessary columns. Instead, focus on the explicit rules for each normal form and stop once those rules are met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part a\n",
    "In words, address the following points regarding 1st normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the original data that prevent it from meeting the requirements for 1st normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 1st normal form. If this schema has multiple tables, give each table a name that indicates what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part a Answer:\n",
    "There are only a couple problems with the originial data that prevents it from meeting the requirment for 1st normal form. Non-Atomic values in the, \"prior_conditions\", which stores lists and Inconsistent values, the column, \"drug_cost\", mixes both integers and decimals\n",
    "\n",
    "\n",
    "To bring the data into 1NF, the main change is with the prior_conditions column. Right now it stores multiple conditions in a single cell, which is non-atomic. To fix this, those conditions should be split out into their own table so that each row contains just one patient–condition pairing. The prescriptions table would then keep all the single valued attributes like patient information, drug details, physician, and hospital.\n",
    "\n",
    "The result is two tables: one for prescriptions (one row per prescription event) and one for patient conditions (one row per condition per patient). The prescriptions table will have the same number of rows as the original sheet, while the conditions table will have more rows since each condition gets its own record. This reorganization ensures that every column contains only one value and the schema now meets the requirements for 1NF.\n",
    "\n",
    "Tables 1NF:\n",
    "Prescriptions (patient info, prescribed_drug, drug details, physician, hospital, etc.)\n",
    "PatientConditions (patient_id, condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "In words, address the following points regarding 2nd normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the 1st normal form data that prevent it from meeting the requirements for 2nd normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 2nd normal form. If this schema has multiple tables, give each table a name that connects to what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part b Answer:\n",
    "\n",
    "The problem lies in the use of a composite natural key like (patient_name, DOB, prescribed_drug) for the big table, which creates partial dependencies.\n",
    "Patient attributes (patient_sex, patient_insurance) depend only on (patient_name, DOB). Drug attributes (DrugMaker, DrugCost) depend only on prescribed_drug. Physician/hospital attributes depend only on PrescribingPhysician. This violates 2NF because non-key columns depend on part of the composite key instead of the full key.\n",
    "\n",
    "To fix this and meet 2NF, the data should be reorganized into separate tables so that each table’s non-key attributes depend only on the whole key.\n",
    "\n",
    "Tables after 2NF:\n",
    "Patients (patient_id, name, DOB, sex, insurance)\n",
    "Drugs (drug_id, name, drug_maker, drug_cost)\n",
    "Physicians (physician_id, name, medschool, years_experience, hospital_id)\n",
    "Hospitals (hospital_id, hospital_name, hospital_location)\n",
    "Prescriptions (prescription_id, patient_id, drug_id, physician_id)\n",
    "PatientConditions (patient_id, condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "In words, address the following points regarding 3rd normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the 2nd normal form data that prevent it from meeting the requirements for 3rd normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 3rd normal form. If this schema has multiple tables, give each table a name that connects to what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part c Answer:\n",
    "\n",
    "The schema set up in 2NF still has transitive dependencies keeping it from meeting 3NF requirements, where non-key attributes depend on other non-key attributes rather than directly on the primary key.\n",
    "In the Drugs table, if drug_maker determines drug_cost (e.g., pricing is tied to the maker), then drug_cost depends transitively on the key.\n",
    "In the Physicians table, the hospital and hospital_location should not depend directly on the physician — they depend on the hospital itself.\n",
    "This violates 3NF because non-key attributes must not depend on other non-key attributes.\n",
    "\n",
    "To fix this, we separate those dependencies into their own tables:\n",
    "Patients (patient_id, name, DOB, sex, insurance)\n",
    "Conditions (condition_id, condition_name)\n",
    "PatientConditions (patient_id, condition_id)\n",
    "Drugs (drug_id, name, maker_id)\n",
    "DrugMakers (maker_id, maker_name, pricing_info if needed)\n",
    "Physicians (physician_id, name, medschool, years_experience, hospital_id)\n",
    "Hospitals (hospital_id, hospital_name, hospital_location)\n",
    "Prescriptions (prescription_id, patient_id, drug_id, physician_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Display all of the tables in their entirety from the version of the data you designed for part (c) to meet the requirements of 3rd normal form. \n",
    "\n",
    "You can use several different methods to display these tables. You can download the tables to CSV format then upload them into your notebook using `pd.read_csv()`, or you can represent them using Markdown table syntax using a tool such as  https://www.tablesgenerator.com/markdown_tables, or you can include them as screenshot images if you want. (Please don't just provide external links however, as that will slow down our grading)\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Patients ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>patient_sex</th>\n",
       "      <th>patient_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nkemdilim Arendonk</td>\n",
       "      <td>2/21/1962</td>\n",
       "      <td>M</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Raniero Coumans</td>\n",
       "      <td>8/15/1990</td>\n",
       "      <td>M</td>\n",
       "      <td>Cigna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id        patient_name date_of_birth patient_sex patient_insurance\n",
       "0           1  Nkemdilim Arendonk     2/21/1962           M             Aetna\n",
       "1           2     Raniero Coumans     8/15/1990           M             Cigna"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conditions ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_id</th>\n",
       "      <th>condition_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Pneumonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Diabetes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition_id condition_name\n",
       "0           1.0      Pneumonia\n",
       "1           2.0       Diabetes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PatientConditions ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>condition_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  condition_id\n",
       "0           1             1\n",
       "1           1             2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DrugMakers ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maker_id</th>\n",
       "      <th>drug_maker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>USAntibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pfizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maker_id     drug_maker\n",
       "0         1  USAntibiotics\n",
       "1         2         Pfizer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Drugs ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_id</th>\n",
       "      <th>drug_name</th>\n",
       "      <th>maker_id</th>\n",
       "      <th>drug_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amoxil</td>\n",
       "      <td>1</td>\n",
       "      <td>14.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Micronase</td>\n",
       "      <td>2</td>\n",
       "      <td>20.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drug_id  drug_name  maker_id  drug_cost\n",
       "0        1     Amoxil         1      14.62\n",
       "1        2  Micronase         2      20.55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hospitals ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_id</th>\n",
       "      <th>hospital</th>\n",
       "      <th>hospital_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UPMC Presbyterian Shadyside</td>\n",
       "      <td>Pittsburgh, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Northwestern Memorial Hospital</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hospital_id                        hospital hospital_location\n",
       "0            1     UPMC Presbyterian Shadyside    Pittsburgh, PA\n",
       "1            2  Northwestern Memorial Hospital       Chicago, IL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Physicians ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>physician_id</th>\n",
       "      <th>prescribing_physician</th>\n",
       "      <th>physician_medschool</th>\n",
       "      <th>physician_years_experience</th>\n",
       "      <th>hospital_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Earnest Caro</td>\n",
       "      <td>University of California (Irvine)</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pamela English</td>\n",
       "      <td>University of Michigan</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   physician_id prescribing_physician                physician_medschool  \\\n",
       "0             1          Earnest Caro  University of California (Irvine)   \n",
       "1             2        Pamela English             University of Michigan   \n",
       "\n",
       "   physician_years_experience  hospital_id  \n",
       "0                          14            1  \n",
       "1                          29            2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prescriptions ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prescription_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>physician_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prescription_id  patient_id  drug_id  physician_id\n",
       "0                1           1        1             1\n",
       "1                2           1        2             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "patients = pd.read_csv(\"Patients.csv\")\n",
    "conditions = pd.read_csv(\"Conditions.csv\")\n",
    "patient_conditions = pd.read_csv(\"PatientConditions.csv\")\n",
    "drugmakers = pd.read_csv(\"DrugMakers.csv\")\n",
    "drugs = pd.read_csv(\"Drugs.csv\")\n",
    "hospitals = pd.read_csv(\"Hospitals.csv\")\n",
    "physicians = pd.read_csv(\"Physicians.csv\")\n",
    "prescriptions = pd.read_csv(\"Prescriptions.csv\")\n",
    "\n",
    "# Display all tables\n",
    "print(\"=== Patients ===\")\n",
    "display(patients.head(2))\n",
    "\n",
    "print(\"=== Conditions ===\")\n",
    "display(conditions.head(2))\n",
    "\n",
    "print(\"=== PatientConditions ===\")\n",
    "display(patient_conditions.head(2))\n",
    "\n",
    "print(\"=== DrugMakers ===\")\n",
    "display(drugmakers.head(2))\n",
    "\n",
    "print(\"=== Drugs ===\")\n",
    "display(drugs.head(2))\n",
    "\n",
    "print(\"=== Hospitals ===\")\n",
    "display(hospitals.head(2))\n",
    "\n",
    "print(\"=== Physicians ===\")\n",
    "display(physicians.head(2))\n",
    "\n",
    "print(\"=== Prescriptions ===\")\n",
    "display(prescriptions.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "For this problem, you will be documenting and a building database that contains the entire collected works of Shakespeare.\n",
    "\n",
    "<img src=\"https://image.cagle.com/178551/750/178551.png\" width=\"300\" alt='Shakespeare Twitter comic'>\n",
    "\n",
    "The data were collected by [Catherine Devlin](https://github.com/catherinedevlin/opensourceshakespeare) for https://opensourceshakespeare.org/. The database will have five tables, and they will be brought into your Python environment as `pandas` dataframes in 3rd normal form  by running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'https://github.com/jkropko/DS-6001/raw/master/localdata/'\n",
    "works = pd.read_csv(repo + 'Works.csv')\n",
    "characters = pd.read_csv(repo + 'Characters.csv')\n",
    "chapters = pd.read_csv(repo + 'Chapters.csv')\n",
    "paragraphs = pd.read_csv(repo + 'Paragraphs.csv')\n",
    "\n",
    "# convert column names to lowercase (needed for postgreSQL to work properly)\n",
    "characters.columns = characters.columns.str.lower() \n",
    "chapters.columns = chapters.columns.str.lower()\n",
    "paragraphs.columns = paragraphs.columns.str.lower()\n",
    "works.columns = works.columns.str.lower()\n",
    "\n",
    "# works in the characters tables is a comma separated list. \n",
    "# Break it out into multiple rows in a new table\n",
    "charworks = characters[['charid', 'works']]\n",
    "charworks.loc[:,'works'] = charworks['works'].str.split(',')\n",
    "charworks = charworks.explode('works')\n",
    "charworks = charworks.rename({'works':'workid'}, axis=1)\n",
    "characters = characters.drop('works', axis=1)\n",
    "\n",
    "#Remove empty rows\n",
    "chapters = chapters.query(\"~chapterid.isnull()\")\n",
    "paragraphs = paragraphs.query(\"~paragraphid.isnull()\")\n",
    "charworks = charworks.query(\"~workid.isnull()\")\n",
    "\n",
    "# Add chapterid to paragraphs\n",
    "paragraphs = pd.merge(paragraphs, \n",
    "                      chapters.drop('description', axis=1),\n",
    "                      how='inner', \n",
    "                      on=['workid', 'section', 'chapter'])\n",
    "\n",
    "#Remove unnecessary columns\n",
    "paragraphs = paragraphs.drop(['paragraphtype', 'section', 'chapter'], \n",
    "                             axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five tables are:\n",
    "\n",
    "**works**: One row per work authored by Shakespeare, with columns:\n",
    "* `workid`: (primary key) a unique ID without spaces or special characters for the work\n",
    "* `title`: the title, such as \"Twelfth Night\"\n",
    "* `longtitle`: a longer title, if there is one, such as \"Twelfth Night, Or What You Will\"\n",
    "* `date`: year of publication\n",
    "* `genretype`: `t` is a tragedy, such as *Romeo and Juliet* and *Hamlet*; `c` is a comedy, such as *A Midsummer Night's Dream* and *As You Like It*; `h` is a history, such as *Henry V* and *Richard III*; `s` refers to Shakespeare's sonnets; `p` is a narrative (non-sonnet) poem, such as *Venus and Adonis* and *Passionate Pilgrim*\n",
    "* `notes`: Column for notes from the database maintainer, currently all `NaN`\n",
    "* `source`: whether the text was originally downloaded from the [Moby Project lexicon](https://en.wikipedia.org/wiki/Moby_Project) or [Project Gutenberg](https://www.gutenberg.org/). \n",
    "* `totalwords`: Total words in the work\n",
    "* `totalparagraphs`: Total number of lines of dialogue for plays, or stanzas for poems\n",
    "\n",
    "**characters**: One row per character that appears in at least one work by Shakespeare. Some characters, such as Antony or Henry IV, appear in multiple works. Columns:\n",
    "* `charid`: (primary key) a unique ID for a character\n",
    "* `charname`: character's name (some characters are different but have the same name, such as the First Musician in Othello and the First Musician in Romeo and Juliet). For poems, the character is \"Poet\" \n",
    "* `abbrev`: an abbreviation of the character's name, if needed for reference to some other analyses\n",
    "* `description`: a longer description of who the character is, if available\n",
    "* `speechcount`: number of lines of dialogue delivered by the character throughout the works the character appears in\n",
    "\n",
    "**chapters**: One row for every unique scene in a play, or for every distinct poem in a collection of poems. Columns:\n",
    "* `workid`: a unique ID without spaces or special characters for the work\n",
    "* `chapterid`: (primary key) a unique ID for the scene/poem\n",
    "* `section`: the scene/poem number\n",
    "* `chapter`: the act number, if available\n",
    "* `description`: short description of where the scene takes place, for plays\n",
    "\n",
    "**paragraphs**: One row for every line of dialogue that appears in a Shakespeare play, or for every distinct poem in a collection of poems. Columns:\n",
    "* `workid`: a unique ID without spaces or special characters for the work\n",
    "* `paragraphid`: (primary key) a unique ID for the line of dialogue/poem\n",
    "* `paragraphnum`: the position of the paragraph within the ordered list of paragraphs within a chapter\n",
    "* `charid`: the unique ID of the character delivering the line of dialogue/poem\n",
    "* `plaintext`: the text of the dialogue/poem\n",
    "* `phonetictext`: the text of the dialogue/poem in phonetic text, useful for training computers to generate audio of this spoken text\n",
    "* `stemtext`: the stems of the words in the text, useful for text analyses such as sentiment analysis\n",
    "* `charcount`: number of characters in the line\n",
    "* `wordcount`: number of words in the line\n",
    "* `chapterid`: unique ID for the scene/poem\n",
    "\n",
    "**charworks**: One row for every unique combination of character and play. Most characters appear once, but some (such as Antony or Henry IV) appear multiple times. Columns\n",
    "* `charid`: (primary key) unique ID for the character\n",
    "* `workid`: (primary key) unique ID for the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "Please refer to the [textbook's discussion of ER diagrams](https://jkropko.github.io/surfing-the-data-pipeline/ch6.html#entity-relationship-diagrams) as you complete this problem.\n",
    "\n",
    "For each of the following pairs of tables, write\n",
    "1. Which column or columns the tables should be joined on\n",
    "2. A description of your reasoning, in plain language, for whether each table matches to one or many rows in the other\n",
    "3. Whether there is a one-to-one, one-to-many, many-to-one, or many-to-many relationship between the tables\n",
    "4. Write the database markup language (DBML) code to represent this kind of a relationship. \n",
    "\n",
    "If we are connecting two tables named `table_a` and `table_b` on a column named `joincolumn` in each table, then the DBML syntax is \n",
    "\n",
    "* `Ref: table_a.joincolumn - table_b.joincolumn` if the relationship from `table_a` to `table_b` is one-to-one\n",
    "\n",
    "* `Ref: table_a.joincolumn < table_b.joincolumn` if the relationship from `table_a` to `table_b` is one-to-many\n",
    "\n",
    "* `Ref: table_a.joincolumn > table_b.joincolumn` if the relationship from `table_a` to `table_b` is many-to-one\n",
    "\n",
    "* `Ref: table_a.joincolumn <> table_b.joincolumn` if the relationship from `table_a` to `table_b` is many-to-many\n",
    "\n",
    "If there are multiple columns that must be joined on, provide the DBML code for each of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem i\n",
    "**charworks** and **works** [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. charworks.workid = works.workid\n",
    "\n",
    "2. Each row in charworks ties a character to a specific work.\n",
    "\n",
    "3. many-to-one from charworks to works (a work can appear in many charworks rows; each charworks row belongs to one work).\n",
    "\n",
    "4. Ref: charworks.workid > works.workid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem ii\n",
    "**characters** and **charworks** [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. characters.charid = charworks.charid\n",
    "\n",
    "2. charworks is the bridge table linking a character to works.\n",
    "\n",
    "3. one-to-many from characters to charworks (a character can be linked to many rows in charworks).\n",
    "\n",
    "4. Ref: characters.charid < charworks.charid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem iii\n",
    "**works** and **paragraphs** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. works.workid = paragraphs.workid\n",
    "\n",
    "2. Every paragraph belongs to exactly one work.\n",
    "\n",
    "3. one-to-many from works to paragraphs.\n",
    "\n",
    "4. Ref: works.workid < paragraphs.workid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem iv\n",
    "**chapters** and **works** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. chapters.workid = works.workid\n",
    "\n",
    "2. Chapters are contained within a work.\n",
    "\n",
    "3. many-to-one from chapters to works (one work has many chapters).\n",
    "\n",
    "4. Ref: chapters.workid > works.workid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem v\n",
    "**paragraphs** and **chapters** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (paragraphs.workid, paragraphs.chapterid) = (chapters.workid, chapters.chapterid)\n",
    "\n",
    "2. A chapter id can repeat across different works, so both keys are needed.\n",
    "\n",
    "3. Many-to-one from paragraphs to chapters.\n",
    "\n",
    "4. Ref: paragraphs.(workid, chapterid) > chapters.(workid, chapterid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem vi\n",
    "**characters** and **paragraphs** (count groups that might say a line in unison, like Chorus or Witches, as one character) [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. characters.charid = paragraphs.charid\n",
    "\n",
    "2. A speaking paragraph is attributed to one character id (or one shared “group” id).\n",
    "\n",
    "3. one-to-many from characters to paragraphs.\n",
    "\n",
    "4. Ref: characters.charid < paragraphs.charid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "\n",
    "Please find the \"Using dbdiagram.io and dbdocs.io to Document Your Database\" page on Canvas. It is available under Modules and \"Getting Started with the Tools We'll Be Using\". Read the page and watch the video about using dbdocs.io and dbdiagram.io to document a relational database with an ER diagram.\n",
    "\n",
    "Use https://dbdiagram.io to create an ER diagram for the five Shakespeare data tables, then publish this diagram to a stable URL by pressing the \"Publish to dbdocs\" button. Paste a link here to your ER diagram on DBdocs.io. Then paste your DBML code from dbdiagrams.io into the box below.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "You can use the `pandas_df_to_dbml()` function, defined below, to generate database markup language (DBML) code for each table that also lists each column's data type, then paste this code into dbdiagram.io (use `print()` around the output to see the correct formatting):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_df_to_dbml(df: pd.DataFrame, table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame to a DBML string.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to convert.\n",
    "        table_name: The name of the table in the DBML schema.\n",
    "\n",
    "    Returns:\n",
    "        A DBML string representing the DataFrame schema.\n",
    "    \"\"\"\n",
    "\n",
    "    dbml_string = f\"Table {table_name} {{\\n\"\n",
    "\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        dbml_type = map_pandas_dtype_to_dbml_type(column_type)\n",
    "        dbml_string += f\"  {column_name} {dbml_type}\\n\"\n",
    "\n",
    "    dbml_string += \"}\\n\"\n",
    "    return dbml_string\n",
    "\n",
    "def map_pandas_dtype_to_dbml_type(dtype) -> str:\n",
    "    \"\"\"Maps a pandas dtype to a DBML type.\"\"\"\n",
    "    dtype_name = str(dtype)\n",
    "    if \"int\" in dtype_name:\n",
    "      return \"int\"\n",
    "    if \"float\" in dtype_name:\n",
    "      return \"float\"\n",
    "    if \"datetime\" in dtype_name:\n",
    "        return \"datetime\"\n",
    "    return \"varchar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the syntax [pk] after a column name and data type to designate the columns that are primary keys in each table. In this case the primary keys are \n",
    "\n",
    "| Table      | Primary Key column(s) |\n",
    "|------------|-----------------------|\n",
    "| works      | workid                |\n",
    "| characters | charid                |\n",
    "| paragraphs | paragraphid           |\n",
    "| chapters   | chapterid             |\n",
    "| charworks  | charid, workid        |\n",
    "\n",
    "To draw the lines linking one table to another, include the `Ref:` syntax that you wrote for your answers to part (a).\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the link to my tables \n",
    "\n",
    "https://dbdocs.io/matorres0925/Lab6_MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Make sure you get the .ipynb file for this lab from the module 6 page on Canvas\n",
    "\n",
    "Then when you double-click this box, you'll see three backticks before and after this text. Leave those alone\n",
    " \n",
    "Type your code here, between the backticks\n",
    "\n",
    "\n",
    "Table works {\n",
    "  workid varchar [pk]      \n",
    "  title varchar\n",
    "  longtitle varchar\n",
    "  date varchar\n",
    "  genretype varchar\n",
    "  notes varchar\n",
    "  source varchar\n",
    "  totalwords int\n",
    "  totalparagraphs int\n",
    "}\n",
    "\n",
    "Table characters {\n",
    "  charid varchar [pk]       \n",
    "  charname varchar\n",
    "  abbr varchar\n",
    "  description varchar\n",
    "  speechcount int\n",
    "}\n",
    "\n",
    "Table chapters {\n",
    "  workid varchar\n",
    "  chapterid varchar\n",
    "  section int\n",
    "  chapter int\n",
    "  description varchar\n",
    "\n",
    "  Indexes {\n",
    "    (workid, chapterid) [pk]\n",
    "  }\n",
    "}\n",
    "\n",
    "Table paragraphs {\n",
    "  paragraphid varchar [pk] \n",
    "  workid varchar\n",
    "  charid varchar\n",
    "  chapterid varchar\n",
    "  para int\n",
    "  paragraph varchar\n",
    "  plain varchar\n",
    "  phonetic varchar\n",
    "  stemtext varchar\n",
    "  charcount int\n",
    "  wordcount int\n",
    "  chapterref varchar\n",
    "}\n",
    "\n",
    "Table charworks {\n",
    "  charid varchar\n",
    "  workid varchar\n",
    "\n",
    "  Indexes {\n",
    "    (charid, workid) [pk]    \n",
    "  }\n",
    "}\n",
    "\n",
    "/* Relationships (IE/crow’s-feet) */\n",
    "Ref: charworks.workid > works.workid\n",
    "Ref: characters.charid < charworks.charid\n",
    "Ref: works.workid < paragraphs.workid\n",
    "Ref: chapters.workid > works.workid\n",
    "Ref: paragraphs.(workid, chapterid) > chapters.(workid, chapterid)\n",
    "Ref: characters.charid < paragraphs.charid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "For this problem, you will use the five dataframes that comprise the Shakespeare database you used in problem 3 to initialize local databases using SQlite, MySQL, and PostgreSQL. \n",
    "\n",
    "All of the methods for initializing and connecting to databases in Python are listed in the [textbook](https://jkropko.github.io/surfing-the-data-pipeline/ch6.html#working-with-databases-in-python). Note that there are differences in code that depend on whether you are using SQlite, MySQL, or PostgreSQL. \n",
    "\n",
    "For creating databases on the MySQL and PostgreSQL servers, you will connect directly to the database server and use the `.cursor()` method to interact with it. But once the database exists, please use the `pd.to_sql()` and `pd.read_sql_query()` methods from `pandas` and the `create_engine()` method from `sqlalchemy` (and not the `.cursor()` approach) to add data to or retrieve data from the database. \n",
    "\n",
    "Before attempting this problem, please make sure your work for problem 1 runs without error, which will ensure your docker containers for MySQL and PostgreSQL are ready to use (SQlite does not require external software and can run without Docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Initialize a new Shakespeare database using SQlite via the `sqlite3` package. Next use the `pd.to_sql()` method to add the five Shakespeare dataframes to this database. Then, to prove that this worked, issue the following SQL query to the database which should display a dataframe listing all characters from Shakespeare's plays with more than 200 lines of dialogue (and the Poet too):\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m chapters   = pd.read_csv(repo + \u001b[33m\"\u001b[39m\u001b[33mChapters.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m paragraphs = pd.read_csv(repo + \u001b[33m\"\u001b[39m\u001b[33mParagraphs.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m charworks  = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCharWorks.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m conn = sqlite3.connect(\u001b[33m\"\u001b[39m\u001b[33mshakespeare.sqlite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m works.to_sql(\u001b[33m\"\u001b[39m\u001b[33mworks\u001b[39m\u001b[33m\"\u001b[39m, conn, if_exists=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:495\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    494\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:604\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:533\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    532\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ds6001/lib/python3.13/urllib/request.py:613\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "repo = \"https://raw.githubusercontent.com/jkropko/DS-6001/master/localdata/\"\n",
    "\n",
    "works      = pd.read_csv(repo + \"Works.csv\")\n",
    "characters = pd.read_csv(repo + \"Characters.csv\")\n",
    "chapters   = pd.read_csv(repo + \"Chapters.csv\")\n",
    "paragraphs = pd.read_csv(repo + \"Paragraphs.csv\")\n",
    "charworks  = pd.read_csv(repo + \"CharWorks.csv\")\n",
    "\n",
    "conn = sqlite3.connect(\"shakespeare.sqlite\")\n",
    "works.to_sql(\"works\", conn, if_exists=\"replace\", index=False)\n",
    "characters.to_sql(\"characters\", conn, if_exists=\"replace\", index=False)\n",
    "chapters.to_sql(\"chapters\", conn, if_exists=\"replace\", index=False)\n",
    "paragraphs.to_sql(\"paragraphs\", conn, if_exists=\"replace\", index=False)\n",
    "charworks.to_sql(\"charworks\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200;\n",
    "\"\"\"\n",
    "display(pd.read_sql_query(q, conn))\n",
    "conn.commit(); conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Use the `dotenv` package to import your MySQL database password into your Python environment (don't expose this password in your code). Then use this password to initialize a new Shakespeare database using MySQL and the `mysql.connector` package. \n",
    "\n",
    "Next, use the `create_engine()` method from `sqlalchemy` and the `pd.to_sql()` method to add the five Shakespeare dataframes to this database. Then, to prove that this worked, use the `pd.read_sql_query()` method to issue the following SQL query to the database\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "Use the `dotenv` package to import your PostgreSQL database password into your Python environment (don't expose this password in your code). Then use this password to initialize a new Shakespeare database using PostgreSQL and the `psycopg` package. (For PostgreSQL, you will need to set the `.autocommit` attribute of Python variable that connects to the PostgreSQL server to `True`, or you will receive cryptic errors.)\n",
    "\n",
    "Next, use the `create_engine()` method from `sqlalchemy` and the `pd.to_sql()` method to add the five Shakespeare dataframes to this database (don't worry if a negative number is displayed. This is a known issue with PostgreSQL in Python but is not indicative of a problem). Then, to prove that this worked, use the `pd.read_sql_query()` method to issue the following SQL query to the database\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "(If you see an error like `ObjectInUse: database \"shakespeare\" is being accessed by other users DETAIL:  There is 1 other session using the database.`, go to the terminal, type Control+C, then `docker compose down`, then `docker compose up`, then try again.)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "[Colin Mitchell](http://muffinlabs.com/) is a web-developer and artist who has a bunch of cool projects that play with what data can do on the internet. One of his projects is [Today in History](https://history.muffinlabs.com/), which provides an API to access all the Wikipedia pages for historical events that happened on this day in JSON format. The records in this JSON are stored in the `['data']['events']` path. Here's the first listing for today (the day I ran this code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '768',\n",
       " 'text': 'Carloman I and Charlemagne are crowned kings of the Franks.',\n",
       " 'html': '768 - <a href=\"https://wikipedia.org/wiki/Carloman_I\" title=\"Carloman I\">Carloman I</a> and <a href=\"https://wikipedia.org/wiki/Charlemagne\" title=\"Charlemagne\">Charlemagne</a> are crowned kings of the Franks.',\n",
       " 'no_year_html': '<a href=\"https://wikipedia.org/wiki/Carloman_I\" title=\"Carloman I\">Carloman I</a> and <a href=\"https://wikipedia.org/wiki/Charlemagne\" title=\"Charlemagne\">Charlemagne</a> are crowned kings of the Franks.',\n",
       " 'links': [{'title': 'Carloman I',\n",
       "   'link': 'https://wikipedia.org/wiki/Carloman_I'},\n",
       "  {'title': 'Charlemagne', 'link': 'https://wikipedia.org/wiki/Charlemagne'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = requests.get(\"https://history.muffinlabs.com/date\")\n",
    "history_json = json.loads(history.text)\n",
    "events = history_json['data']['Events']\n",
    "events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the count of total events for the day I ran this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, you will use MongoDB and the `pymongo` package to create a local document store NoSQL database containing these historical events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "First, check that your work for problem 1 is still running without error. If so, then you have a MongoDB server running inside a Docker container. \n",
    "\n",
    "Use `pymongo` to connect to the local MongoDB client, create a database named \"history\" and a collection within that database named \"today\". Because you will probably be running this code several times as you work, debugging as you go along, it's useful write code that deletes any existing \"today\" collections before creating a new \"today\" collection.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Insert all of the records in `events` into this collection. [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Issue the following query to find all of the records whose text contain the word \"Virginia\":\n",
    "```\n",
    "query = {\n",
    "    \"text\":{\n",
    "        \"$regex\": 'England'\n",
    "    }\n",
    "}\n",
    "```\n",
    "If there are no results that contain the word \"England\", choose a different word like \"China\" or \"war\". Display the count of the number of documents that match this query, display the output of the query, and generate a JSON formatted variable containing the output. [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "Once you are finished working with databases, clear up the space on your computer by going to the terminal that you used to launch the Docker containers, press CONTROL + C on your keyboard to stop the containers, then type `docker compose down` to disconnect the volumes and networks. It's a good idea to make a practice out of doing these steps when you finish working with databases.\n",
    "\n",
    "This problem isn't graded, and no need to write anything. But please do this anyway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
